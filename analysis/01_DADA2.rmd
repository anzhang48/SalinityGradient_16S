---
title: "Infer ASVs with DADA2"
author: "Andy Zhang"
date: "`r Sys.Date()`"
output: 
  html_document:
  toc: yes
  toc_float:
      collapsed: no
      smooth_scroll: yes
      toc_depth: 3
editor_options: 
  chunk_output_type: console
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      fig.align = "center",
                      fig.path = "../figures/01_DADA2/")
```

# Before you start

## Set my seed

```{r set-seed}
# Any number can be chosen
set.seed(895820392)

```

# Goals of this file

1.  Use row fastq files and generate quality plots to assess quiality of reads.
2.  Filter out and trim out bad sequences and bases from our sequencing files.
3.  Write out fastq files with high quality sequences
4.  Evaluate the quality from our filter and trim
5.  Infer Errors on forward and reverse reads individually
6.  Identified ASVs on forward and reverse reads separately, using error model.
7.  Merge forward and reverse ASVs into "contigous ASVs".
8.  Generate the ASV count table ('otu_table' input for phyloseq)

## Output that we need:

1.  ASV count table 'otu_table'
2.  Taxonomy table 'tax_table'
3.  Sample Information: 'sample_data' track the reads lost throughout the DADA2 workflow

# Load Libraries

```{r load-libraies}
#install.packages("devtools")
library(devtools)

#devtools::install_github("benjjneb/dada2")
library(dada2)

#install.packages("tidyverse")
library(tidyverse)

#install.packages("patchwork")
library(patchwork)
```

# Load Data

```{r load-data}
# Set Raw fastq path to the raw sequencing files
# Path to the fastq files
raw_fastq_path <- "data/01_DADA2/01_raw_gzippedfastqs"
raw_fastq_path

# What files are in this path? Intuition Check
list.files(raw_fastq_path)

# How many files are there?
str(list.files(raw_fastq_path))

# Create vector of forward reads
raw_forward_reads <- list.files(raw_fastq_path, pattern = "R1_001.fastq.gz", full.names = TRUE)
head(raw_forward_reads)
str(raw_forward_reads)

# Create a vector of reverse reads
raw_reverse_reads <- list.files(raw_fastq_path, pattern = "R2_001.fastq.gz", full.names = TRUE)
# Intution Check
head(raw_reverse_reads)
```

# Quality Plots

```{r raw-quality-plot}
# Randomly Select two samples to evaluate
random_samples <- sample(1:length(raw_reverse_reads), size = 2)
random_samples

# Calculate and plot quality of these two samples
plotQualityProfile(raw_forward_reads[random_samples])+
                     labs(title = "Raw Forward Quality Plot")
plotQualityProfile(raw_reverse_reads[random_samples])+
                     labs(title = "Raw Reverse Quality Plot")

```

# Prepare a place holder for filtered reads

```{r prep-filtered-sequences}
# Vector of our samples, extract sample name from files
samples <- sapply(strsplit(basename(raw_forward_reads), "_"), '[',1)
# Intuition Check
head(samples)
  
# Place filtered reads into filtered_fastq_path
filtered_fastqs_path <- "data/01_DADA2/02_filtered_fastqs"

# Create 2 variables: filtered_F, filtered_R
filtered_forward_reads <- 
  file.path(filtered_fastqs_path, paste0(samples, "_R1_filtered.fastq.gz"))
length(filtered_forward_reads)
head(filtered_forward_reads)

# Same thing for reverse reads
filtered_reverse_reads <- 
  file.path(filtered_fastqs_path, paste0(samples, "_R2_filtered.fastq.gz"))
length(filtered_reverse_reads)
head(filtered_reverse_reads)
```

# Filter and Trim Reads

Parameters of filter and trim **DEPEND ON THE DATASET**

-   'maxN' = number of N bases. Remove all Ns from the data.
-   'maxEE' = Quality Filtering threshold, applied to expected error. Here, if there's 2 expected errors. It's ok. But more than 2. Thrown away the sequence. Two values, first is for the forward reads; second is for reverse reads.
-   'trimleft' Number of nucleotides to remove at the start of each read.
-   'truncQ' Truncate reads after truncLen bases. Reads shorter than this are discarded.

```{r filter-and-trim}
# Assign vector to filtered reads
# trim out poor bases, first 3 bps on F reads
# write out filtered fastq files

filtered_reads <- filterAndTrim(fwd = raw_forward_reads, filt = filtered_forward_reads,
              rev = raw_reverse_reads, filt.rev = filtered_reverse_reads,
              maxN = 0, maxEE = c(2,2), trimLeft = 3,
              truncQ = 2, rm.phix = TRUE, compress = TRUE)
# Intuition Check
length(filtered_reads)
```

# Graphing Filter and Trim Quality Plots

```{r filter-trim-quality-plot}



```

# Aggregated Trimmed Plots

```{r}
# Aggregate QC Plots
#install and library patchwork
plotQualityProfile(filtered_forward_reads, aggregate = TRUE) +
  plotQualityProfile(filtered_reverse_reads, aggregate = TRUE)

```

## Stats on read output from "filterAndTrim"

```{r FilterTrim-stats}
filtered_df <- as.data.frame(filtered_reads)
head(filtered_df)

# Statistics on our Reads
filtered_df %>%
  reframe(median_reads_in = median(reads.in),
          median_reads_out = median(reads.out),
          median_percent_retained = (median(reads.out/median(reads.in))))
```

#Error Modeling

**Note** Run separately on each Illumina dataset.

```{r learn-errors}

# Forward Reads
error_forward_reads <- 
  learnErrors(filtered_forward_reads) 
#multithreaded = TRUE"

# Reverse Reads
error_reverse_reads <- 
  learnErrors(filtered_reverse_reads) 
#multithreaded = TRUE"

# Plot reverse
plotErrors(error_reverse_reads, nominalQ = TRUE)+
  labs(title = "Reverse Read Error Model")
```

# Infer ASVs

Note this is happening seperately on the forward and reverse reads. This is unique to DADA2.

```{r Infer ASVs}
#Infer forward ASVs
dada_forward <- dada(filtered_forward_reads,
                     err = error_forward_reads)
                     #multithread = TRUE
#Infer reverse ASVs
dada_reverse <- dada(filtered_reverse_reads,
                     err = error_reverse_reads)
                     #multithread = TRUE

```

# Merge Forward and Reverse ASVs

```{r merge forward and reverse asvs}
# merge forward and reverse ASVs
merged_ASVs <- mergePairs(dada_forward, filtered_forward_reads,
                          dada_reverse, filtered_reverse_reads,
                          verbose = TRUE)

# Evaluate the Output
typeof(merged_ASVs)
length(merged_ASVs)
names(merged_ASVs)
```

# Generate ASV count table

```{r generate-ASV-Table}
# Create ASV count table
row_ASV_table <- makeSequenceTable(merged_ASVs)

# Write out the file to data/DADA2

```

# Session Information

```{r session-info}
# Ensure reproducibility
devtools::session_info()
```
